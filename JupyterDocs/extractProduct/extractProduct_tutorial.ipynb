{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Layers of ARIA standard GUNW products\n",
    "\n",
    "**Author**: Brett A. Buzzanga, David Bekaert - Jet Propulsion Laboratory\n",
    "\n",
    "This notebook documents the methodology used for extracting data and metadata layers of ARIA Geocoded UNWrapped interferogram (**GUNW**) products. It provides command-line and python examples that include visualizations and an application. \n",
    "\n",
    "Specifically, we detail the methodology used in extracting:\n",
    "- product bounding box, amplitude, and coherence,\n",
    "- imaging geometry, \n",
    "- unwrapped phase. \n",
    "\n",
    "We then show examples which use the tools to extract based on:\n",
    "- the full union of the acquisition frames (no bounding box),\n",
    "- a bounding box of SNWE coordinates in decimal degrees,\n",
    "- a bounding box in the form of a vector shapefile.\n",
    "\n",
    "Finally, we demonstrate how to generate a deformation map in a georeferenced coordinate system. \n",
    "\n",
    "    \n",
    "<div class=\"alert alert-danger\">\n",
    "Both the initial setup (Section A) and download of the data (Section B) should be run at the start of the notebook. However, the numbered sections do not need to be run in numerical order. </b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Potential Errors:</b> \n",
    "If the driver is not captured as \"netCDF/Network Common Data Format\" verify GDAL version is at least 2.5.\n",
    "\n",
    "The folder containing the ARIA python tools must be in your path. If it's not you can add it like so:\n",
    "    \n",
    "`os.environ[\"PATH\"] += os.pathsep + \"path_to_tools\"`\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep A. Initial setup of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary python libraries, setting the paths we will use to access and store data, and defining a function to plot a layer.\n",
    "\n",
    "**This step only needs to be ran at the very beginning, but it must be run every time the notebook is restarted.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home directory:  /Users/bb/Software_InSAR/ARIA-tools-docs_git/JupyterDocs/extractProduct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## add the aria tools to your path\n",
    "path_tools = os.path.join(os.path.expanduser('~'), 'Software_InSAR', 'ARIA-tools_git', 'tools', 'python')\n",
    "os.sys.path.append(path_tools)\n",
    "os.environ[\"PATH\"] += os.pathsep + path_tools\n",
    "\n",
    "try:\n",
    "    import shapefile_util as shputil\n",
    "except:\n",
    "    raise Exception('ARIA-tools missing from path')\n",
    "\n",
    "# Enable GDAL/OGR exceptions\n",
    "gdal.UseExceptions()\n",
    "\n",
    "## Defining the home and data directories at the processing location\n",
    "home_dir = os.getcwd()\n",
    "tutorial_home_dir = os.path.abspath(os.path.join(home_dir, \"\"))\n",
    "data_dir = os.path.join(tutorial_home_dir, 'data')\n",
    "supp_dir = os.path.join(tutorial_home_dir, 'support_docs')\n",
    "print(\"home directory: \", tutorial_home_dir)\n",
    "\n",
    "# data file\n",
    "fileNames = [os.path.join(data_dir, i) for i in ['S1-GUNW-A-R-124-tops-20190304_20190226-042953-19661N_17420N-PP-720e-v2_0_1.nc', 'S1-GUNW-A-R-124-tops-20190304_20190226-043020-21158N_19086N-PP-5415-v2_0_1.nc']]\n",
    "\n",
    "\n",
    "# generate all the folders in case they do not exist yet\n",
    "if not os.path.exists(tutorial_home_dir):\n",
    "    os.makedirs(tutorial_home_dir)\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "os.chdir(tutorial_home_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layers(path_layers, **kwargs):\n",
    "    \"\"\" path_layers can be a string or a list of up to 4 layers\"\"\"\n",
    "    if not isinstance(path_layers, list):\n",
    "        path_layers = [path_layers]\n",
    "    n_layers = len(path_layers)\n",
    "    if len(path_layers) > n_layers:\n",
    "        raise Exception('Can only pass <= 4 layers')\n",
    "    else:\n",
    "        n_cols = 2 if n_layers == 4 else n_layers \n",
    "        n_rows = 2 if n_layers == 4 else 1\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(16,9))\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes  = np.array(axes)\n",
    "    axe       = axes.ravel()\n",
    "    cmap = plt.cm.Greys_r\n",
    "    cmap.set_under('black')\n",
    "    \n",
    "    for i, ax in enumerate(axe):\n",
    "        lay_type = os.path.dirname(path_layers[i])\n",
    "        ds  = gdal.Open(path_layers[i], gdal.GA_ReadOnly)\n",
    "        arr = ds.ReadAsArray()\n",
    "        if lay_type.endswith('amplitude'):\n",
    "            ax.imshow(arr, cmap=cmap, vmax=2000, **kwargs)# you may have to adjust vmax for diff scenes\n",
    "        elif lay_type.endswith('coherence'):\n",
    "            ax.imshow(arr, cmap=cmap, **kwargs)\n",
    "        elif lay_type.endswith('incidenceAngle'):\n",
    "            ax.imshow(arr, cmap=cmap, vmin=np.quantile(arr, 0.53), **kwargs)\n",
    "        elif lay_type.endswith('bPerpendicular'):\n",
    "            # ax.imshow(arr, cmap=cmap, vmin=arr.min(), vmax=arr.max(), **kwargs)\n",
    "            raise Exception('use productPlot.py to view baseline information')\n",
    "            \n",
    "            \n",
    "        ax.grid(False)\n",
    "        ax.set_title(os.path.basename(lay_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep B: Download the data\n",
    "The GUNW products are packages as netCDF4 files and include both data and meta-data layers such as interferogram amplitude, filtered unwrapped phase, filtered coherence, connected components, perpendicular and parallel baselines, incidence, azimuth and look angles. A detailed overview of the ARIA GUNW product with respect to processing, formatting, sampling, and data layers can be found on the ARIA website.\n",
    "\n",
    "Products can be downloaded from the ARIA-products page and ASF DAAC vertex page under beta products. If you know the product filename you can construct the download link by appending the filename to the following URL: https://grfn.asf.alaska.edu/door/download/\n",
    "\n",
    "For our dataset we are focusing on two adjacent frames from ascending track 124 of Sentinel-1 with an interferogram generated between 20180423 and 20180505. The two frames span 19.661N to 17.420N and 21.158N to 19.086N, respectively, thus it is clear they are adjacent in that they overlap bewteen 19.661N and 19.086N. Together, the two frames cover the entirety of the Big Island of Hawai'i. The interferogram dates capture a coseismic earthquake that occured on May 4, 2018 in the southeastern corner of the island (Fig. 1).\n",
    "\n",
    "<img src=\"support_docs/Hawaii_shakemap.png\" alt=\"region\" width=\"500\">\n",
    "\n",
    "**Fig. 1** Shakemap of coseismic earthquake that occured on May 4, 2018 in the southeastern corner of the Hawai'in Big Island.\n",
    "\n",
    "\n",
    "Given that we already know the product filenames we can download them by constructing the download URLs:\n",
    "- https://grfn.asf.alaska.edu/door/download/S1-GUNW-A-R-124-tops-20190304_20190226-042953-19661N_17420N-PP-720e-v2_0_1.nc\n",
    " \n",
    "- https://grfn.asf.alaska.edu/door/download/S1-GUNW-A-R-124-tops-20190304_20190226-043020-21158N_19086N-PP-5415-v2_0_1.nc\n",
    "\n",
    "Make sure to place them in the data_dir you defined above, (e.g. ./data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview ofÂ extractProduct.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we show how to call extractProduct.py and view the help. In the help is a description for each possible input argument. The effects of these arguments will be demonstrated in more detail in the sections that follow. \n",
    "A digital elevation model (DEM) is required for processing. The user can specify the location to a custom one, or pass `-d Download` to download on the fly from NASA SRTM mission.\n",
    "\n",
    "Multiple ARIA files (`-f`) and layers (`-l`) can be specified by separating the filenames by a **space** and/or  layer names with a **comma** (`-l 'azimuth,coherence'`).\n",
    "Additionally, ARIA files can be specified using a wildcard, e.g. (`-f 'S1*'`), and/or all layers can be requested,  (`-l all`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: extractProduct.py [-h] -f IMGFILE [-w WORKDIR] [-l LAYERS] [-d DEMFILE]\r\n",
      "                         [-p PROJECTION] [-b BBOX] [-m MASK]\r\n",
      "                         [-of OUTPUTFORMAT] [-croptounion] [-verbose]\r\n",
      "\r\n",
      "Program to extract data and meta-data layers from ARIA standard GUNW products.\r\n",
      "Program will handle cropping/stiching when needed.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -f IMGFILE, --file IMGFILE\r\n",
      "                        ARIA file\r\n",
      "  -w WORKDIR, --workdir WORKDIR\r\n",
      "                        Specify directory to deposit all outputs. Default is\r\n",
      "                        local directory where script is launched.\r\n",
      "  -l LAYERS, --layers LAYERS\r\n",
      "                        Specify layers to extract as a comma deliminated list\r\n",
      "                        bounded by single quotes. Allowed keys are:\r\n",
      "                        \"unwrappedPhase\", \"coherence\", \"amplitude\",\r\n",
      "                        \"bPerpendicular\", \"bParallel\", \"incidenceAngle\",\r\n",
      "                        \"lookAngle\",\"azimuthAngle\". If \"all\" is specified,\r\n",
      "                        then all layers are extracted. If blank, will only\r\n",
      "                        extract bounding box.\r\n",
      "  -d DEMFILE, --demfile DEMFILE\r\n",
      "                        DEM file. To download new DEM, specify \"Download\".\r\n",
      "  -p PROJECTION, --projection PROJECTION\r\n",
      "                        projection for DEM. By default WGS84.\r\n",
      "  -b BBOX, --bbox BBOX  Provide either valid shapefile or Lat/Lon Bounding\r\n",
      "                        SNWE. -- Example : '19 20 -99.5 -98.5'\r\n",
      "  -m MASK, --mask MASK  Provide valid mask file.\r\n",
      "  -of OUTPUTFORMAT, --outputFormat OUTPUTFORMAT\r\n",
      "                        GDAL compatible output format (e.g., \"ENVI\", \"GTiff\").\r\n",
      "                        By default files are generated virtually except for\r\n",
      "                        \"bPerpendicular\", \"bParallel\", \"incidenceAngle\",\r\n",
      "                        \"lookAngle\",\"azimuthAngle\", \"unwrappedPhase\" as these\r\n",
      "                        are require either DEM intersection or corrections to\r\n",
      "                        be applied\r\n",
      "  -croptounion, --croptounion\r\n",
      "                        If turned on, IFGs cropped to bounds based off of\r\n",
      "                        union and bbox (if specified). Program defaults to\r\n",
      "                        crop all IFGs to bounds based off of common\r\n",
      "                        intersection and bbox (if specified).\r\n",
      "  -verbose, --verbose   Toggle verbose mode on.\r\n"
     ]
    }
   ],
   "source": [
    "cmd = '-h'\n",
    "!extractProduct.py {cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Spatio-temporal Configuration\n",
    "Before more advanced operations are performed, the program determines the alignment of the requested frames in space and time. \n",
    "The user can input a list of interferograms acquired over one or more satellite paths, as long as frames overlap within each path (see [Prep B](#Dowload-the-Data)).\n",
    "The default is to separate the data in time, then take the spatial **union** of the scenes in the *along-track* direction, and the **intersection** in the  *across-track* direction\n",
    "\n",
    "If the user wants to use the full union of the frames regardless of alignment, or if the satellite paths do not intersect, **pass the `--croptounion` flag**.\n",
    "Note that along the equator track number gets incremented in the product name but the data itself is still continuous? \n",
    "\n",
    "The following plot shows the three valid spatial configurations for pairs of interferograms:\n",
    "<img src=\"support_docs/spatial_config.png\" alt=\"spatial_configuration\" width=\"1000\">\n",
    " \n",
    "**Fig. 2** In Panel A, only frames from the same path are used, so the program will default to use the union, outlined in maroon. In Panel B, the default will return the intersection of the frames, shaded in light green; the union resulting from -croptounion is outlined in maroon. For Panel C, the default will fail because there is no intersection; however, passing -croptounion will successfully result in the maroon outline.\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Warning:</b> The user is responsible for ensuring for there is an overlap between frames in the along-track direction. \n",
    "</div>\n",
    "$%We can then plot the shapefile of our bounding box as follows:$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cropping \n",
    "The program then crops based on the user-defined bounding box, if given. \n",
    "The user can specify a rectangular bounding box of SNWE decimal degree coordinates as `-b 'S N W E'` or a vector shapefile `-b path_to_file.shp` (note the lack of quotes surrounding the path argument). \n",
    "\n",
    "Examples of the different cropping scenarios are shown in [Section 3](#3).\n",
    "\n",
    "All layers are cropped and/or stitched using GDAL. The data in the overlap regions are used for interpolation and for computing offsets between connected components and unwrapped phase in adjacent frames (for full details see [Section 2.3](#Extracting-Unwrapped-Phase)).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>DAVID:</b> Is it necessary to show more figures here? - I think it's self explanatory with the plots above, and the different cases are below in examples\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Full Resolution Meta-Data\n",
    "Unlike many other interferometric? processing softwares, this program extracts the full resolution ImagingGeometry meta-data, including perpendicular and parallel baselines, and azimuth, incidence and look angles.\n",
    "These data are packaged in the GUNW products as 3D data cubes referenced to several height levels.\n",
    "extractProduct.py will intersect the given DEM, interpolating between the reference levels to the height of the DEM for each point in 2D space.\n",
    "$%3D grid main data compression. Its smooth on a 3D and can be done on a course resolution. topography enforces the high resolution$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  Output Format\n",
    "The program will write data as virtual (.vrt) files unless the user requests a different format, such as ENVI or ISCE.\n",
    "These files contain processing instructions in an XML structure, rather then the processed data itself. \n",
    "This paradigm improves both processing efficiency and use of disk space.\n",
    "However, certain layers, such as the unwrapped phase and imaging geometry, will need to be physically written to disk for mathematical manipulation.\n",
    "\n",
    "*Data for each layer will be saved in a separate subdirectories relative to the working directory (`-w`) as the name of the layer. Within the subdirectory, the data and metadata are saved with the interferogram pair dates as the basename.*\n",
    "\n",
    "In the [Examples](#Examples), we specify the working directory as **./support_docs**; therefore, the coherence raster will be saved as **./support_docs/coherence/20180505_20180423** with corresponding VRT **20180505_20180423.vrt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Two-Dimensional Stitching\n",
    "The **amplitude** and **coherence** layers are stitched together in a straightforard manner using the python bindings to `gdalwarp`. The data are resampled using the default nearest-neighbor algorithm, which begins calculation in the overlap region and propagates outwards.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>DAVID:</b> This is the best I could figure out. Proposed the question to Sim and waiting to hear back. I originally thought it would leave the values outside the overlap region unchanged, but I looked into that and it does not.\n",
    "    </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting ImagingGeometry layers \n",
    "The ImagingGeometry layers we provide in high resolution 2D grids (lon, lat), are: bPerpendicular, bParallel, incidenceAngle, lookAngle, and azimuthAngle.\n",
    "We first stitch together the full 3D grids (lon, lat, height) shipped with the **GUNW** product using the default nearest neighbor resampling algorithm of `gdalwarp`.\n",
    "The longitude and latitude coordinate spacing arises from the multilooking (averaging of neighboring pixels) in azimuth and range.\n",
    "The height direction consists of the ImagingGeometry layer data calculated at several reference height levels.\n",
    "We next interpolate the data over the rectangular 2D (lon, lat) coordinates of the stitched frame using [Scipy's implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RectBivariateSpline.html) of a bicubic spline.\n",
    "Then at each point in 2D space we interpolate the heights vertically, use the [`scipy.interpolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html).\n",
    "Next, we interpolate the 2D layer data from the coarser GUNW spacing to that of the DEM.\n",
    "Finally we intersect the full 3D interpolated grid with the DEM to return the 2D values that correspond to the correct topography.\n",
    "A sketch is shown in Figure 3:\n",
    "<img src=\"support_docs/interp.png\" alt=\"interp_sketch\" width=\"300\" >\n",
    " \n",
    "**Fig. 3** A sketch of the extraction process for ImagingGeometry layers. An artificial full 3D grid is shown with vertices in grey sampled at lon, lat, height. A DEM that indicates topography is shown in green, with paths to the satellite as black lines. \n",
    "The diamond markers show the values of intersection that compose the 2D grid written to disk.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>DAVID:</b> I am not sure if the description of coarser to GUNW spacing (and the multilooking part) to that of the DEM is correct. I am sure it goes from coarser (about 37 points in lat/lon, which correspond to the GUNW lat/lon), to several thousand, which correspond to the raster size of the DEM after its been fit to the frames bounding box. I also don't get how the 'intersection' happens. It happens on this line `interpolator(np.stack((np.flip(dem.ReadAsArray(), axis=0), lat, lon), axis=-1))`, does the interp multiply by zeros somewhere to cancel the dimension? \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Extracting Unwrapped Phase\n",
    "Explain the minization procedure in overlap regions between product.\n",
    "Explain what happens to the connected component file and how they are corrected\n",
    "Could show the bounding box and then color the union, say we will compute the residual between the products in the intersect region\n",
    "\n",
    "total of connected components of used; offset is applied and only overlap section of master is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 View the product outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not request any layers from `extractProduct.py`, the program will only create geoJSON files that show the boundary of each time period. \n",
    "Without a bounding box, it will default to the **full intersection** between the two scenes.\n",
    "We will save all files created to the 'support_documents' directory by passing `-w supp_dir` and force the program to download a DEM on the fly with the flag `-d Download`'. \n",
    "\n",
    "The products, generated as geoJSON files in the productsBoundingBox subdirectory include:\n",
    "- the full stitched geometry for **each interferogram** with the acquisition dates forming the filename, e.g.  **20180505_20180423.shp**\n",
    "- the interesect/union and or crop, as requested with additional arguments as **productBoundingBox.shp**\n",
    "\n",
    "We can then view the outline saved in the geoJSON file, with the **plot_shapefile** function within **shapefile_utils.py**. \n",
    "Note that this function can be used for plotting polygon and multi-polygon vector geometry in many files that can be opened with GDAL, including ESRI shape files.\n",
    "<div class=\"alert alert-danger\">\n",
    "    <b>DAVID:</b> Possibly just explain the -d download and -w supp_dir here, instead of in every box?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd    = \"-f '{}' -d Download -w {}\".format(' '.join(fileNames), supp_dir)\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "!extractProduct.py {cmd}\n",
    "\n",
    "## plot the results\n",
    "# shputil.plot_shapefile(os.path.join(supp_dir, 'productBoundingBox', '20190304_20190226.shp'))\n",
    "# shputil.plot_shapefile(os.path.join(supp_dir, 'productBoundingBox', 'productBoundingBox.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only using data acquired over one satellite path, the plots are identical; that is, the fully stitched scene is equivalent to the intersection of the two scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 Extract layer without a bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to extract a layer without using a bounding box over the scenes over Hawai'i described in [Prep B](#Dowload-the-Data)).\n",
    "Since we are only using data acquired over one satellite path, this is equivalent to the union of the two scenes.\n",
    "\n",
    "We will force the program to download a DEM on the fly with the flag `-d Download`'.\n",
    "We will extract and view the amplitude and coherence using the plotting function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['amplitude', 'coherence']\n",
    "# cmd    = \"-f '{}' -l '{}' -d Download -w {}\".format(' '.join(fileNames), ','.join(layers), supp_dir,)\n",
    "cmd    = \"-f '{}' -l all -d Download -w {}\".format(' '.join(fileNames), supp_dir)\n",
    "\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "!extractProduct.py {cmd}\n",
    "\n",
    "# plot\n",
    "layer_dest = [os.path.join(supp_dir, layer, '20190304_20190226') for layer in layers]\n",
    "plot_layers(layer_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amplitude plot on the left shows the magnitude of the stitched interferometric pair. Weaker (darker) values indicate weaker backscatter of the radar signal.\n",
    "\n",
    "The coherence plot on the right is a measure of the similarity between observations in the two SAR images that compose the interferometric pair and ranges from 0 (no coherence) to 1 (identically coherent). Capillary waves superimposed on larger scale fluctuations cause water to have little coherence from one acquisition to the next, as easily viewed here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extract layer with SNWE coordinates\n",
    "To focus analysis, the user can specify a rectangular bounding box of SNWE coordinates as a space delimited list surround by quotes.\n",
    "Here we focus on the Big Island of Hawai'i using coordinates `SNWE = '18.8 20.3 -156.1 -154.5'`. As in the previous example, we will download the DEM on the fly by passing `-d Download` and view both the amplitude and coherence. Again, the data will be saved in **supp_dir** we set above, with separate subdirectories for each layer name. Within the subdirectory, the data and metadata are saved with the interferogram pair dates as the basename. Any existing layers will be overwritten.\n",
    "\n",
    "Note that if the user ran the previous cell, he/she does not need to specify a DEM; the program will find it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNWE = '18.8 20.3 -156.1 -154.8'\n",
    "cmd    = \"-f '{}' -l '{}' -d Download -w {} --bbox '{}'\".format(' '.join(fileNames), ','.join(layers), supp_dir, SNWE)\n",
    "\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "!extractProduct.py {cmd}\n",
    "\n",
    "# set paths and view the plots\n",
    "layer_dest = [os.path.join(supp_dir, layer, '20190304_20190226') for layer in layers]\n",
    "plot_layers(layer_dest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3  Extract layer using a shapefile\n",
    "Another option for focusing on a region is to pass a user defined data. **The data must be a vector dataset of polygons, readable by GDAL***. \n",
    "These include ESRI shapefiles and geoJSON structures.\n",
    "If it is not referenced to WGS84 you can pass a the projection with `-p your_proj` (untested).\n",
    "\n",
    "Here we crop to the eastern corner of the Big Island of Hawai'i, which contains the epicenter of the earthquake. \n",
    "We will extract and view the incidence angle.\n",
    "As in the previous example, we will download the DEM on the fly by passing `-d Download`\n",
    "Once more, the data will be saved in **supp_dir** we set above, with separate subdirectories for each layer name. Within the subdirectory, the data and metadata are saved with the interferogram pair dates as the basename. Any existing layers will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['incidenceAngle']\n",
    "shp    = os.path.join(supp_dir, 'Big_Island', 'Hawaii_Poly_wgs84.shp')\n",
    "cmd    = \"-f '{}' -l '{}' -w {} -d Download -b {}\".format(' '.join(fileNames), ','.join(layers), supp_dir, shp)\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "!extractProduct.py {cmd}\n",
    "layer_dest = [os.path.join(supp_dir, layer, '20190304_20190226') for layer in layers]\n",
    "plot_layers(layer_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incidence angles generally decrease from East to west, from a high of 43.55 to 41.86."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generation of Georeferenced Displacement Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract incidence angle, heading together using the list option\n",
    "# Extract the wavelength from the product (this is not supported by default)\n",
    "# Generate the ENU conversion and use function to generate the products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW IS OLD/EXTRA CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Merging of the two scenes\n",
    "Here we demonstrate how to extract the **amplitude** data of the scenes.\n",
    "Inputting both scenes to extracProduct.py program will automatically merge them.\n",
    "We also show how to automatically download the required digital elevation model (DEM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = ['amplitude']\n",
    "ifgs   = '{} {}'.format(path_ifg1, path_ifg2)\n",
    "# ifgs   = '{}'.format(path_ifg1)\n",
    "# cmd    = \"--file '{}' --layers '{}' --workdir {} --demfile=Download\".format(ifgs, ','.join(layers), res_dir)\n",
    "cmd    = \"--file '{}' --layers '{}' --workdir {}\".format(ifgs, ','.join(layers), res_dir)\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "print ('Finished extracting {}'.format(layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. View Extracted Amplitude\n",
    "The extraction tool creates a raster with basename of the interferogram dates and no extension in res_dir/amplitude.\n",
    "It also creates a .vrt file (for use with GDAL, see the GDAL Notebook[LINK]), and an .xml file that is neede for ISCE[LINK] tools such as mdx.py.\n",
    "\n",
    "\n",
    "Here we demonstrate a quick look at the resulting amplitude data using GDAL and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ifg = os.path.join(res_dir, 'amplitude', '20190304_20190226')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. View Product Bounding Box\n",
    "Additionally, 2 geojson files (.shp) are created in res_dir/productBoundingBox.\n",
    "The one with basename of the interferogram dates is bounding box of each scene? and the productBoundingBox_total.shp is total product? (will check later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_bbox  = os.path.join(res_dir, 'productBoundingBox', '20190304_20190226.shp')\n",
    "# product_total = os.path.join(res_dir, 'productBoundingBox', 'productBoundingBox_total.shp')\n",
    "\n",
    "# plot_shapefile(product_bbox)\n",
    "# plot_shapefile(product_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crop the merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 We'll now crop out most of the water around the Big Island using a lat/lon bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNWE = '18.8 20.3 -156.1 -154.5'\n",
    "layers = ['amplitude']\n",
    "ifgs   = '{} {}'.format(path_ifg1, path_ifg2)\n",
    "cmd    = \"-f '{}' -l '{}' -w {} --bbox '{}' \".format(ifgs, ','.join(layers), res_dir, SNWE)\n",
    "print ('Running extractProduct.py {}'.format(cmd))\n",
    "# !extractProduct.py {cmd}\n",
    "print ('Finished cropping {}'.format(layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 View Bounding Box 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_1 = os.path.join(supp_dir, 'productBoundingBox', '20161003_20160822.shp')\n",
    "bbox_2 = os.path.join(supp_dir, 'productBoundingBox', '20170206_20170107.shp')\n",
    "bbox_3 = os.path.join(supp_dir, 'productBoundingBox', '20190304_20190226.shp')\n",
    "bbox_4 = os.path.join(supp_dir, 'productBoundingBox', '20160729_20160611.shp')\n",
    "bbox_5 = os.path.join(supp_dir, 'productBoundingBox', '20160810_20160506.shp')\n",
    "bbox_total = os.path.join(supp_dir, 'productBoundingBox', 'productBoundingBox_total.shp')\n",
    "# bbox_list = [bbox_1, bbox_2, bbox_3, bbox_total]\n",
    "bbox_list = [bbox_4, bbox_5, bbox_total]\n",
    "for bbox in bbox_list:\n",
    "#     shputil.plot_shapefile(bbox)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_1 = os.path.join(supp_dir, 'productBoundingBox', '20161003_20160822.shp')\n",
    "bbox_2 = os.path.join(supp_dir, 'productBoundingBox', '20170206_20170107.shp')\n",
    "bbox_4 = os.path.join(supp_dir, 'productBoundingBox', '20160729_20160611.shp')\n",
    "bbox_5 = os.path.join(supp_dir, 'productBoundingBox', '20160810_20160506.shp')\n",
    "bbox_total = os.path.join(supp_dir, 'productBoundingBox', 'productBoundingBox_total.shp')\n",
    "# bbox_list = [bbox_1, bbox_2, bbox_3, bbox_total]\n",
    "bbox_list = [bbox_4, bbox_5, bbox_total]\n",
    "for bbox in bbox_list:\n",
    "#     shputil.plot_shapefile(bbox)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 View cropped Amplitude\n",
    "The cropping routine will have produced a shapefile with our bounding box (user_bbox.shp) in res_dir, and overwritten the files in the res_dir/amplitude.\n",
    "\n",
    "Let's take a quick look at the cropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ifg1 = os.path.join(supp_dir, 'coherence', '20161003_20160822.vrt')\n",
    "path_ifg2 = os.path.join(supp_dir, 'coherence', '20170206_20170107.vrt')\n",
    "path_ifg3 = os.path.join(supp_dir, 'coherence', '20190304_20190226.vrt')\n",
    "path_ifg4 = os.path.join(supp_dir, 'coherence', '20190304_20190226.vrt')\n",
    "# plot_layers([path_ifg1, path_ifg2, path_ifg3, path_ifg4])\n",
    "plot_layers([path_ifg3, path_ifg4])\n",
    "\n",
    "# path_ifg5 = os.path.join(supp_dir, 'bPerpendicular', '20160729_20160611.vrt')\n",
    "# path_ifg6 = os.path.join(supp_dir, 'bPerpendicular', '20160810_20160506.vrt')\n",
    "#plot_layers([path_ifg5, path_ifg6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://osmdata.openstreetmap.de/data/coastlines.html\n",
    "# ifgs   = '{} {}'.format(path_ifg1, path_ifg2)\n",
    "# cmd    = \"-f '{}' -l '{}' -w {} --bbox '{}'\".format(ifgs, ','.join(layers), res_dir, shp)\n",
    "# print ('Running extractProduct.py {}'.format(cmd))\n",
    "# !extractProduct.py {cmd}\n",
    "# print ('Finished cropping with shapefile {}'.format(layers))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
